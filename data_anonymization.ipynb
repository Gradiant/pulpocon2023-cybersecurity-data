{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a2594b",
   "metadata": {},
   "source": [
    "# Anonimización Pulpocon 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f4384",
   "metadata": {},
   "source": [
    "## Datos y privacidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc4d07",
   "metadata": {},
   "source": [
    "En la era del Big Data, la información es uno de los bienes más preciados, y esto es así porque podemos sacar valor de ella gracias a la gran cantidad de información disponible y a las técnicas de análisis de datos y machine learning que se han desarrollado.\n",
    "\n",
    "Este valor pasa por ofrecer más y mejores productos y servicios o por mejorar la eficiencia y eficacia de la organización, pero también por garantizar su seguridad y la de sus clientes.\n",
    "\n",
    "La seguridad no solo hace ahorrar tiempo y dinero en solventar los incidentes que se puedan producir, sino que también aumentará la confianza y satisfacción con nuestra organización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87501d1b",
   "metadata": {},
   "source": [
    "En este contexto, la privacidad de los dueños de la información disponible está en peligro. Se hace necesario controlar y proteger la información personal, todos aquellos datos que sobre una persona identificable por un atributo o la combinación de varios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8884e1f",
   "metadata": {},
   "source": [
    "La UE estableció el GDPR para brindar protección sobre nuestros datos personales. Exige un consentimiento claro y específico para tratar datos personales, da más control a los ciudadanos sobre sus datos y obliga a las empresas a proteger esa información. Además, tiene carácter retroactivo, los que prácticamente imposibilita el procesamiento de datos históricos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979ed5d",
   "metadata": {},
   "source": [
    "Por suerte, tenemos herramientas para proteger datos personales, tratando de mantener su valor, hasta el punto en el que es imposible, o casi, identificar los dueños originales de los datos, haciendo que no sean datos personales y, por tanto, ya no es necesario acogerse a la medidas específicas de la GDPR: **la anonimización**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485891f",
   "metadata": {},
   "source": [
    "### Dataset de detección de fraude bancario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6829f72d",
   "metadata": {},
   "source": [
    "Para esta parte de anonimización vamos a utilizar un dataset bancario sobre los préstamos a sus clientes.\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/mishra5001/credit-card\n",
    "\n",
    "De momento, no nos vamos a centrar demasiado en el significado real de estos datos, ya que la anonimización sería un paso previo a su procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9066c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leemos el csv y vemos que pinta tiene\n",
    "\n",
    "data = pd.read_csv('data/application_data_selected.csv')\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921f3d5",
   "metadata": {},
   "source": [
    "El dataset original tiene muchísimas columnas, así que he seleccionado unas pocas que se entienden bien para que el taller sea más ágil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb03428",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Descomentamos esto si utilizamos el dataset original\n",
    "\n",
    "# selected_cols = [\n",
    "#     'SK_ID_CURR',\n",
    "#     'AMT_INCOME_TOTAL',\n",
    "#     'AMT_CREDIT',\n",
    "#     'DAYS_BIRTH',\n",
    "#     'DAYS_EMPLOYED',\n",
    "#     'TARGET',\n",
    "#  ]\n",
    "\n",
    "# data = data[selected_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c316256",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5af091",
   "metadata": {},
   "source": [
    "Descripción de la columnas:\n",
    "\n",
    "- SK_ID_CURR: id del préstamo.\n",
    "\n",
    "- AMT_INCOME_TOTAL: ingresos del cliente.\n",
    "\n",
    "- AMT_CREDIT: importe del préstamo.\n",
    "\n",
    "- DAYS_BIRTH: edad del cliente en días en el momento de la solicitud.\n",
    "\n",
    "- DAYS_EMPLOYED: cuántos días antes de la solicitud la persona empezó a trabajar.\n",
    "\n",
    "- TARGET: cliente con dificultades de pago (1 - tuvo retrasos en el pago de más de X días en al menos una de las primeras Y cuotas del préstamo, 0 - todos los demás casos\n",
    "\n",
    "Como vemos, contiene información que pueden ayudar a identificar a los clientes, además de información sensible como es la financiera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4701c",
   "metadata": {},
   "source": [
    "Con un profiler, u otras herramientas de visualización, podemos tener una mejor idea de como son nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88bb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Profiling del dataset, muy útil para hacer exploración de datos\n",
    "\n",
    "# prof = ProfileReport(data)\n",
    "# prof.to_file(output_file='reports/application_data_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681df064",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plots para ver la distribución de los datos\n",
    "\n",
    "# sns.pairplot(data.drop('SK_ID_CURR', axis=1).sample(10000, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c3a556",
   "metadata": {},
   "source": [
    "Tenemos que decidir qué columnas anonimizamos y, realmente, todas serían susceptibles, puesto que son datos personales. Sin embargo:\n",
    "\n",
    "- Los identificadores solo sirven para identificar préstamos, no van a aportar información si los anonimizamos. Ya nos encargaremos más tarde de ellos.\n",
    "\n",
    "- TARGET es una variable binaria, dificil de anonimizar sin romper su significado, ya que tiene dos posibles valores. Además, parece ser la variable a predecir en el futuro porcesamiento de los datos, así que trataremos de no tocarla si no es necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b28102",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_cols = [\n",
    "    'AMT_INCOME_TOTAL',\n",
    "    'AMT_CREDIT',\n",
    "    'DAYS_BIRTH',\n",
    "    'DAYS_EMPLOYED'\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428a669",
   "metadata": {},
   "source": [
    "## Anonimización\n",
    "\n",
    "Eliminar o reducir al mínimo el riesgo de identificación de las personas a partir de un conjunto de datos.\n",
    "\n",
    "Se debe asumir que el adversario tiene la mayor cantidad posible de información externa sobre los usuarios, y evaluar así el riesgo de re-identificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55413ec",
   "metadata": {},
   "source": [
    "Los dos principales mecanismos para anonimizar un dataset son:\n",
    "\n",
    "- **Generalización**: se busca agregar los datos de distintos usuarios para evitar su re-identificación.\n",
    "\n",
    "- **Randomización**: se busca añadir una distorsión a los datos, de modo que no se puedan relacionar con su valor original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af44617d",
   "metadata": {},
   "source": [
    "Es necesario tener en cuenta que estas transformaciones de los datos hacen que pierdan parte de su significado original, pero suelen ser necesarias para garantizar una cierta privacidad. Típicamente, cuánto más agresiva la anonimización, más información se perderá, haciendo que los datos pierdan utilidad para futuros usos, como sacar modelos estadísticos de ellos.\n",
    "\n",
    "En estos contextos, es necesario jugar con la privacidad y la utilidad para alcanzar el equilibrio deseado entre ambas, por lo que precisamos de formas de medir ambas variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079f848",
   "metadata": {},
   "source": [
    "### K-anonymity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2bc0e1",
   "metadata": {},
   "source": [
    "Implica ocultar la identidad de los individuos en un conjunto de datos al asegurarse de que cada fila sea indistinguible entre al menos \"k\" otros registros en términos de atributos compartidos, protegiendo así la privacidad mientras se mantiene la utilidad de los datos. Está muy orientada a la privacidad por generalización.\n",
    "\n",
    "\"K\", por tanto, funciona como una métrica de privacidad: cuanto mayor sea \"k\" en nuestro dataset, mayor privacidad tendremos.\n",
    "\n",
    "En la práctica, \"k\" se calcula como el número de veces que se repite en nuestro dataset la combinación de valores de una fila que menos se repite. Sería como hacer un group_by de todas las columnas y ver el grupo que menos registros contiene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa812521",
   "metadata": {},
   "source": [
    "<img src=\"images/kanonymity.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68514d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Función para calcular k-anonimity\n",
    "\n",
    "def k_anonymity(df: pd.DataFrame, return_sizes:bool=False) -> typing.Union[list, int]:\n",
    "    group_sizes = list(df.value_counts())  # group_sizes = list(df.groupby(list(df.columns)).size())\n",
    "    if return_sizes:\n",
    "        return group_sizes\n",
    "    \n",
    "    return min(group_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabff6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_anonymity(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e8bf8e",
   "metadata": {},
   "source": [
    "El \"k\" de nuestro dataset inicial es 1, lo que quiere decir que hay filas cuya combinación de valores es única. Pero era esperable, ya que sabíamos que hay identificadores únicos en la primera columna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8081c5ea",
   "metadata": {},
   "source": [
    "**Primer paso de la anonimización**: eliminar identificadores únicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da31f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('SK_ID_CURR', axis='columns', inplace=True)\n",
    "k_anonymity(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc8d1a",
   "metadata": {},
   "source": [
    "Vemos que sigue sin cumplir k-anonimity, así que será necesario transformar estos datos para garantizar su privacidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5321fc",
   "metadata": {},
   "source": [
    "### MSE (Error cuadrático medio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f4b84",
   "metadata": {},
   "source": [
    "El MSE es una de las métricas más utilizada para medir pérdida de utilidad. Cuanto mayor sea el error, más información perdemos.\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Es importante notar que este error saldrá elevado al cuadrado, para saber el error medio en la escala de nuestros datos será necesario hacer la raíz cuadrada. En la siguiente implementación bastará con pasarle *squared = False*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f98b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Función que calcula el error entre dos datasets\n",
    "\n",
    "def mse_by_col(df1: pd.DataFrame, df2:pd.DataFrame, squared:bool=True) -> pd.Series:\n",
    "    if df1.columns.difference(df2.columns).size > 0 or df1.shape != df2.shape:\n",
    "        raise ValueError('Dataframes must have same shape and columns')\n",
    "    \n",
    "    result = []\n",
    "    cols = df1.columns\n",
    "    \n",
    "    for col in cols:\n",
    "        result.append(mean_squared_error(df1[col], df2[col], squared=squared))\n",
    "        \n",
    "    return pd.Series(result, index=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c38de2",
   "metadata": {},
   "source": [
    "Para facilitar la evaluación de los datos anonimizados, a continuación dejo una función que saca tanto el K como el error con respecto a los datos originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23dc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Función de evaluación\n",
    "\n",
    "def evaluate_anonymization(df_original, df_anonym, squared_error:bool=True) -> None:\n",
    "    group_sizes = k_anonymity(df_anonym, return_sizes=True)\n",
    "    mse_by_columns = mse_by_col(df_original, df_anonym, squared=squared_error)\n",
    "    k = min(group_sizes)\n",
    "    \n",
    "    print('Métrica K del dataset:', k)\n",
    "    print('\\nNúmero de grupos:', len(group_sizes))\n",
    "    if k == 1:\n",
    "        print('\\nNúmero de registros únicos:', group_sizes.count(k))\n",
    "    print('\\n\\nMSE por columnas:')\n",
    "    print(mse_by_columns)\n",
    "    print('\\nMSE promedio:', mse_by_columns.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6b544",
   "metadata": {},
   "source": [
    "### Mecanismos de generalización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c5532",
   "metadata": {},
   "source": [
    "La operaciones de generalización más comunes sobre las columnas de un dataset son clustering, redondeo, categorización y microagregaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05b130",
   "metadata": {},
   "source": [
    "#### K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb303924",
   "metadata": {},
   "source": [
    "K-Means es un algoritmo de clustering no supervisado que agrupa un conjunto de datos en \"k\" grupos basados en sus características similares.\n",
    "\n",
    "Funciona asignando puntos de datos al grupo cuyo centroide (punto medio) esté más cercano, y luego actualiza los centroides al calcular el promedio de los puntos asignados. Este proceso se repite iterativamente hasta que los centroides convergen y los grupos se vuelven estables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f22c047",
   "metadata": {},
   "source": [
    "<img src=\"images/kmeans.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdbf0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generalizar con k-means un dataset\n",
    "\n",
    "def k_means(df:pd.DataFrame, n_clusters:int=8) -> pd.DataFrame:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=40)\n",
    "    kmeans.fit(df)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    return pd.DataFrame(centroids[labels], columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe42ccc",
   "metadata": {},
   "source": [
    "##### Primera forma de generalizar: Kmeans de todo el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd542d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos una copia del dataset original para modificar solo las columnas que queremos generalizar\n",
    "\n",
    "data_kmeans = data.copy()\n",
    "data_kmeans[sensitive_cols] = k_means(data[sensitive_cols], n_clusters=4)\n",
    "data_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ac6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que tal fue la generalización con k-means\n",
    "\n",
    "evaluate_anonymization(data, data_kmeans, squared_error=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a558a",
   "metadata": {},
   "source": [
    "Vaya... esto no era lo que esperaba. Hemos encontrado un registro que no conseguimos agrupar con ningún otro de los 307510, ni usando tan solo 4 grupos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscamos el outlier\n",
    "\n",
    "data_kmeans.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413afec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Localizamos la fila\n",
    "\n",
    "data_kmeans[data_kmeans['AMT_INCOME_TOTAL'] == 1.170000e+08]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadba40",
   "metadata": {},
   "source": [
    "Hemos encontrado un outlier bastante notable en los ingresos totales, así que lo vamos a eliminar y volver a ejecutar K-means, a ver si así no tenemos grupos tan pequeños y mejoramos la privacidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf05240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos el outlier\n",
    "\n",
    "data.drop(12840, axis=0, inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a aplicar k-means\n",
    "\n",
    "data_kmeans = data.copy()\n",
    "data_kmeans[sensitive_cols] = k_means(data[sensitive_cols], n_clusters=4)\n",
    "data_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a evaluar la generalización\n",
    "\n",
    "evaluate_anonymization(data, data_kmeans, squared_error=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d8b53",
   "metadata": {},
   "source": [
    "Podemos ver como tenemos un K de 1603, lo que implica un riesgo de re-identificación de 0.0624%, por lo que la privacidad es bastante alta. Sin embargo, al hacer tan solo 4 grupos de valores en todo el dataset, tenemos mucha pérdida de utilidad. Tal vez haciendo más grupos conseguiríamos mejorar la utilidad sin sacrificar demasiada privacidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26eea9",
   "metadata": {},
   "source": [
    "##### Segunda forma de generalizar: Kmeans por columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que generaliza on k-means un dataset columna a columna\n",
    "\n",
    "def k_means_by_col(df:pd.DataFrame, n_clusters:int=8) -> pd.DataFrame:\n",
    "    new_df = df.copy()\n",
    "    for col in df.columns:\n",
    "        new_df[[col]] = k_means(df[[col]], n_clusters)\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda27a7",
   "metadata": {},
   "source": [
    "Al generalizar por columnas perdemos menos utilidad, pero tenemos muchas posibles combinaciones de valores cuando el número de columnas es muy grande, por lo que es fácil que haya combinaciones únicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e8501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos k-means por columnas\n",
    "\n",
    "data_kmeans_bycol = data.copy()\n",
    "data_kmeans_bycol[sensitive_cols] = k_means_by_col(data[sensitive_cols], n_clusters=4)\n",
    "data_kmeans_bycol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos la generalización por columnas\n",
    "\n",
    "evaluate_anonymization(data, data_kmeans_bycol, squared_error=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bb2006",
   "metadata": {},
   "source": [
    "Parece que en este caso tenemos, más o menos, la mitad de pérdida de información que antes, pero no hemos conseguido cumplir k-anonimity. Hay 9 combinaciones de valores que son únicas. Tal vez haciendo menos grupos en alguna columna lo conseguiríamos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d9d5d",
   "metadata": {},
   "source": [
    "#### Microagregaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d6b46",
   "metadata": {},
   "source": [
    "Es una manera de garantizar k-anonymity que consiste en ir haciendo grupos de \"k\" registros similares de forma iterativa, hasta que todos los registros estén en un grupo de, al menos, ese tamaño."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d201fd3",
   "metadata": {},
   "source": [
    "El algoritmo recibe un conjunto de datos y un *k*. Comienza calculando la fila promedio de todo el dataset. Después, coge la fila que más se aleje de la media (fila *r*) para que sea el centro del primer grupo. A continuación, coge la fila más alejada de *r* (fila *s*) para ser el centro del otro grupo. Al rededor de ambas se forman dos grupos de tamaño *k*, con las *k-1* filas más cercanas a *r* y a *s*. Esos registros se descartan del conjunto de entrada y se vuelve a empezar, hasta que queden menos de 2*k* filas, que forman el último grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929e079d",
   "metadata": {},
   "source": [
    "<img src=\"images/microaggregations.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generalizar con microagregaciones\n",
    "\n",
    "def euclidean_distance(v1, v2):\n",
    "    if type(v1) == pd.DataFrame or type(v2) == pd.DataFrame:\n",
    "        return np.sqrt((v2 - v1).pow(2).sum(axis=1))\n",
    "    else:\n",
    "        return np.sqrt(np.sum((v2 - v1)**2))\n",
    "\n",
    "def microaggregation(df, k):\n",
    "    input_df = df.copy()\n",
    "    output_df = df.copy()\n",
    "    while len(input_df) >= 3*k:\n",
    "        # (a) Compute the average record x˜ of all records in R. The average record is computed attribute-wise.\n",
    "        # (b) Consider the most distant record xr to the average record x˜ using an appropriate distance.\n",
    "        # (c) Find the most distant record xs from the record xr considered in the previous step.\n",
    "        # (d) Form two clusters around xr and xs, respectively. One cluster contains xr and the k −1 records closest to xr. The other cluster contains xs and the k −1 records closest to xs.\n",
    "        # (e) Take as a new dataset R the previous dataset R minus the clusters formed around xr and xs in the last instance of Step 1d\n",
    "        \n",
    "        mean_row = input_df.mean(axis=0)\n",
    "        \n",
    "        # r: Cluster más lejano a la media actual\n",
    "        r_index = euclidean_distance(mean_row, input_df).idxmax()\n",
    "        r_row = input_df.loc[r_index]\n",
    "        r_distances = euclidean_distance(r_row, input_df)\n",
    "        r_cluster_indexes = r_distances.nsmallest(k).index\n",
    "        r_cluster = input_df.loc[r_cluster_indexes]\n",
    "        r_cluster_mean = r_cluster.mean(axis=0)\n",
    "        \n",
    "        # s: Cluster más lejano a r\n",
    "        s_index = r_distances.idxmax()\n",
    "        s_row = input_df.loc[s_index]\n",
    "        s_distances = euclidean_distance(s_row, input_df)\n",
    "        s_cluster_indexes = s_distances.nsmallest(k).index\n",
    "        s_cluster = input_df.loc[s_cluster_indexes]\n",
    "        s_cluster_mean = s_cluster.mean(axis=0)\n",
    "        \n",
    "        # sustituimos en la salida los clusters por su media, y los eliminamos del conjunto de entrada\n",
    "        output_df.loc[r_cluster.index] = r_cluster_mean.values\n",
    "        output_df.loc[s_cluster.index] = s_cluster_mean.values\n",
    "        \n",
    "        input_df.drop(r_cluster.index, axis=0, inplace=True)\n",
    "        input_df.drop(s_cluster.index, axis=0, inplace=True)\n",
    "    \n",
    "    if 3*k-1 > len(input_df) >= 2*k:\n",
    "        # (a) compute the average record x˜ of the remaining records in R\n",
    "        # (b) find the most distant record xr from x˜\n",
    "        # (c) form a cluster containing xr and the k − 1 records closest to xr\n",
    "        # (d) form another cluster containing the rest of records.\n",
    "        \n",
    "        mean_row = input_df.mean(axis=0)\n",
    "        \n",
    "        # r: Cluster más lejano a la media actual\n",
    "        r_index = euclidean_distance(mean_row, input_df).idxmax()\n",
    "        r_row = input_df.loc[r_index]\n",
    "        r_distances = euclidean_distance(r_row, input_df)\n",
    "        r_cluster_indexes = r_distances.nsmallest(k).index\n",
    "        r_cluster = input_df.loc[r_cluster_indexes]\n",
    "        r_cluster_mean = r_cluster.mean(axis=0)\n",
    "        \n",
    "        output_df.loc[r_cluster.index] = r_cluster_mean.values\n",
    "        \n",
    "        input_df.drop(r_cluster.index, axis=0, inplace=True)\n",
    "        \n",
    "        # s: Cluster con los registros restantes\n",
    "        s_cluster = input_df\n",
    "        s_cluster_mean = s_cluster.mean(axis=0)\n",
    "        \n",
    "        output_df.loc[s_cluster.index] = s_cluster_mean.values\n",
    "        \n",
    "        input_df.drop(s_cluster.index, axis=0, inplace=True)\n",
    "        \n",
    "    else:\n",
    "        # form a new cluster with the remaining records\n",
    "        \n",
    "        # Cluster restante\n",
    "        s_cluster = input_df\n",
    "        s_cluster_mean = s_cluster.mean(axis=0)\n",
    "        \n",
    "        output_df.loc[s_cluster.index] = s_cluster_mean.values\n",
    "        \n",
    "        input_df.drop(s_cluster.index, axis=0, inplace=True)\n",
    "        \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos microagregaciones\n",
    "\n",
    "data_microagg = data.copy()\n",
    "data_microagg[sensitive_cols] = microaggregation(data[sensitive_cols], k=500)\n",
    "data_microagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7201a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos la generalización con microagregaciones\n",
    "\n",
    "evaluate_anonymization(data, data_microagg, squared_error=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f85a890",
   "metadata": {},
   "source": [
    "Debemos tener en cuenta que TARGET no la agrupamos, motivo por el que no obtenemos un \"k\" de 500. Si incluyesemos esa columna en las microagregaciones, sí lo conseguiríamos, pero perderíamos utilidad de la variable objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c09968b",
   "metadata": {},
   "source": [
    "**Ejercicio**: Mejorar los resultados privacidad/utilidad de las técnicas vistas. Se puede modificar *k*, el número de clusters y las columnas que se utilizan para agregar. También se puede modificar la parte que querais de los algortimos para hacerlos más rápidos o para reducir la pérdida de información."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
