{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de Anomalías\n",
    "\n",
    "Una **anomalía** es nada más que un punto que presenta desviaciones respecto a lo que es normal, regular o natural. \n",
    "En el ámbito de la ciberseguridad, un punto anómalo en nuestro conjunto de datos (logs de usuarios, tráfico de red, emails...) tenderá a representar un **ataque**, ya que indica un comportamiento distinto al que suele haber en nuestros sistemas. Pero, incluso en el caso de que no fuese un ataque o actividad maliciosa, querremos poder detectar conductas aómalas en nuestra organización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework\n",
    "\n",
    "Vamos a usar pandas para la manipulación de nuestro dataset, y matplotlib, sns y plotly para las visualizaciones de gráficos que podamos necesitar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Nuestro conjunto de datos son registros de transacciones de tarjetas de crédito de distintos usuarios, y nuestro objetivo será detectar las transacciones fraudulentas.\n",
    "\n",
    "El dataset contiene datos reales, pero tiene la particularidad de que sus variables fueron generadas PCA (Principal Component Analysis), una técnica de reducción de dimensiones de datos.\n",
    "Este tipo de métodos se aplican a los datos por varios motivos:\n",
    "\n",
    "- Anonimización. Esta fue la motivación principal de los creadores del dataset. Tras aplicar PCA sobre las variables originales, se obtienen variables nuevas que no nos permiten deducir la información original del conjunto de datos. El inconveniente principal de usar reducción de dimensionalidad para anonimización es la pérdida de interpretabilidad de las variables, pero podemos seguir usando los datos para clasificación, detección de anomalías, etc, mientras mantengamos intacta la variable que indica la clase/etiqueta de cada dato.\n",
    "\n",
    "- Solución del \"curse of dimensionality\". El coste computacional y la dificultad de analizar datos con un montón de variables, a lo que se le llama dimensionalidad alta, es muy elevado. Muchas veces se aplican técnicas como PCA o TSNE para reducir el número de dimensiones/variables de los datos antes de entrenar algoritmos de ML. Estos algoritmos se aplican para eliminar o combinar las variables que tengan correlaciones altas entre sí. De esta forma, pasamos a tener menos columnas en nuestro dataset final, lo cual facilita el entrenamiento de modelos de IA. \n",
    "\n",
    "Además, el uso de nombres de variables abstractos como V1, V2, V3... nos permite reducir nuestro bias hacia los datos, y aumentar así la generalización del modelo. Manteniéndonos agnósticos al dominio de aplicación, nos centramos en la selección y procesado objetivo de los datos para obtener los mejores resultados.\n",
    "\n",
    "https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/creditcard.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las variables son números decimales, ahorrándonos el problema de tener que lidiar con variables categóricas. La única excepción es 'Class', la etiqueta que distingue transacciones normales de las fraudulentas, que es un valor entero (0 o 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En dataset no nos falta ningún valor, si se diese el caso tendríamos que rellenar el campo (usando, por ejemplo, la media) o eliminar la fila correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos un dataset no balanceado, donde la clase 1 (fraude) tiene muchas menos instancias que la clase 0 (datos normales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos generar un report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof = ProfileReport(df)\n",
    "prof.to_file(output_file='reports/creditcard_report.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlación entre variables\n",
    "\n",
    "Antes de aplicar ningún tipo de AI a nuestros datos, podemos representar su correlación lineal para ver si hay algún tipo de influencia entre sus valores.\n",
    "\n",
    "En este caso concreto, veremos que las columnas que empiezan por \"V\" no están relacionadas en absoluto. Esto es un resultado directo de haber sido generadas con PCA, donde queremos poder representar nuestros datos con el mínimo número de dimensiones, por lo que eliminaremos/agruparemos variables que estén relacionadas.\n",
    "\n",
    "La variable Amount no ha sido modificada, por lo que sí tenemos información sobre ella. Por ejemplo, vemos que tiene una correlación negativa bastante fuerte con V2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(df.corr(),\n",
    "    labels=dict(color='Correlación'),\n",
    "    text_auto=True,\n",
    "    color_continuous_scale='tropic',\n",
    "    color_continuous_midpoint=0\n",
    ")\n",
    "fig.update_layout(\n",
    "    title_text='Correlación lineal entre variables',\n",
    "    paper_bgcolor='white',\n",
    "    height=500, width=800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que más nos interesa aquí es ver la correlación entre las variables y la clase (Class). Vemos que no todas influyen de la misma forma, y hay algunas variables que parece que no afectan a la clase en absoluto (de forma lineal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dispersión a pares de las variables\n",
    "\n",
    "Aparte de las correlaciones, podemos representar cómo se distribuyen nuestros puntos para cada par de variables (lo llamamos pairplot).\n",
    "\n",
    "En primer lugar, para reducir el tiempo de cálculo y tener una visualización más clara, creamos un dataframe nuevo con todos los datos anómalas y 3000 muestras de los datos normales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = df[df['Class'] == 0].sample(3000)\n",
    "anomaly = df[df['Class'] == 1]\n",
    "sampled_df = pd.concat([normal, anomaly], ignore_index=True)\n",
    "sampled_df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para empezar, seleccionamos las variables que tengan un nivel de correlación relativamente alto con la columna Class. Por ejemplo, a partir de 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_correlation = 0.10\n",
    "high_corr_cols = list(df.corr()['Class'][abs(df.corr()['Class']) > min_correlation].index)\n",
    "print(f'Variables con correlación > {min_correlation} o < -{min_correlation}: {high_corr_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico representa la distribución de nuestros puntos para ambas clases (Rojo = Normal, Azul Claro = Fraude), para cada par de variables.\n",
    "En la diagonal tenemos histogramas de cada clase para la misma variable. Nos interesa ver cómo de separables son las dos clases para estos conjuntos de variables.\n",
    "\n",
    "Podemos apreciar que, dependiendo del conjunto de variables, hay más o menos superposición entre los puntos. Si pudiesemos separar completamente las dos clases para alguna variable o conjunto de variables, tendríamos un problema de clasificación muy sencillo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(sampled_df[high_corr_cols], \n",
    "             hue=\"Class\", \n",
    "             palette='hls',\n",
    "             plot_kws={\"alpha\": 0.2})\n",
    "g.fig.set_dpi(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasa si comparamos el gráfico anterior al que obtenemos representando variables con muy poca correlación (< 0.05)?\n",
    "\n",
    "Escoged algunas variables y representémoslas en un nuevo pairplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_corr_cols = [] #escoged columnas!\n",
    "g = sns.pairplot(sampled_df[low_corr_cols + ['Class']], \n",
    "             hue='Class', \n",
    "             palette='hls',\n",
    "             plot_kws={'alpha': 0.2})\n",
    "g.fig.set_dpi(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué deberíamos observar en estos gráficos, para las variables de correlación baja?\n",
    "\n",
    "Tanto los grupos de puntos como los histogramas de estas clases se solapan mucho más que en los gráficos del pairplot de arriba (¡algunas son prácticamente indistinguibles!).\n",
    "A priori, esto **podría** indicar que estas variables no le aportarán información valiosa al modelo para distinguir entre la clase normal y anómala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección de anomalías & entrenamiento no supervisado\n",
    "\n",
    "Vamos a centrarnos en técnicas de entrenamiento **no supervisado** para la detección de anomalías. ¿Qué quiere decir \"no supervisado\"? Significa que NO necesitamos etiquetas en nuestro conjunto de datos para su entrenamiento. No parece para tanto, pero es una cualidad extremadamente útil para el tipo de situaciones (¡especialmente en ciberseguridad!) donde queremos aplicar detección de anomalías, porque tiene las siguientes implicaciones:\n",
    "\n",
    "- No tenemos que etiquetar nuestros datos. Normalmente, nosotros no sabemos de antemano cuáles de nuestros correos o logs son maliciosos **antes** de aplicar técnicas de análisis de datos. Es muy dificil etiquetar correctamente este tipo de información, y la mayor parte de organizacione no la tiene disponible. En general, en aplicaciones de ciberseguridad, es muy poco probable que encontremos un dataset etiquetado que se adapte bien y sirva para nuestros objetivos.\n",
    "\n",
    "- Nuestro modelo de aprendizaje no supervisado va a poder detectar **todo tipo** de comportamientos maliciosos, mientras sean anómalos. Incluso si tuviesemos a mano un dataset perfectamente etiquetado, si utilizamos algorimos supervisados es probable que aprenda a detectar muy bien los ataques que aparecen en el dataset, pero esto no tiene por qué extenderse a otros tipos de ataques que puedan suceder en el futuro.\n",
    "\n",
    "### Isolation Forest\n",
    "\n",
    "El Isolation Forest es el modelo de ML que vamos a usar como método de detección de anomalías. Como indica su nombre, es un algoritmo basado en árboles de decisión.\n",
    "Intenta separar o aislar las instancias anómalas a base de hacer particiones sobre nuestro conjunto de datos. En cada árbol de decisión, va seleccionando valores de división aleatorios para cada variable, y divide los datos en dos grupos dependiendo de si tienen valores mayores o menores que el valor de división. Cada grupo crea una rama nueva en el árbol, donde se realizará la siguiente partición. Esto se repite hasta que todos los datos queden aislados, o hasta que se llegue a un nivel de profundidad máximo.\n",
    "\n",
    "Finalmente, la puntuación de anomalía de cada dato se decide en función de la profundidad a la que se encuentra su rama. Los datos anómalos serán más sencillos de separar del resto que los datos normales y necesitarán un número menor de particiones, por tanto se encontrarán en ramas muy poco profundas del árbol. La puntuación o score final dependerá de los valores que se obtengan en todos los árboles de decisión para ese dato.\n",
    "\n",
    "Para separar los datos normales de los anómalos, hace falta definir un threshold o umbral de decisión. Por defecto, el Isolation Forest tiene un umbral de decisión de 0.5, por tanto los datos que obtengan puntuaciones mayores a 0.5 se clasifican como anomalías.\n",
    "\n",
    "En la imagen podemos ver las particiones de distintos árboles del Isolation Forest, con las anomalías representadas en rojo.\n",
    "\n",
    "<img src=\"images/iforest.jpg\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División de los datos\n",
    "\n",
    "Para entrenar y evaluar nuestro modelo, primero dividimos el dataset en los datos que usamos para entrenar (X_train) y los datos que usamos para predecir (X_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[high_corr_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento semi-supervisado\n",
    "\n",
    "Es muy similar al no supervisado, seguimos sin necesitar etiquetas para entrenar el modelo pero asumimos que sólo hay datos normales en nuestro set de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = df[df['Class'] != 1].drop('Class', axis=1)\n",
    "anomalies = df[df['Class'] == 1].drop('Class', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(normal, random_state=22, test_size=0.01) #cambiar test_size para tener más o menos % de datos en X_test\n",
    "y_test = pd.Series([0]*X_test.shape[0])\n",
    "X_test = X_test.append(anomalies)\n",
    "y_test = y_test.append(pd.Series([1]*anomalies.shape[0]))\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "print(f'Tamaño de datos de entrenamiento: {len(X_train)}')\n",
    "print(f'Tamaño de datos de evaluación: {len(X_test)} ({len(anomalies)} anomalías)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento no supervisado\n",
    "\n",
    "No necesitamos etiquetas en nuestro set de entrenamiento, sólo en el de test para evaluar nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(normal, random_state=22, test_size=0.01) #cambiar test_size para tener más o menos % de datos en X_test\n",
    "train_anomalies, test_anomalies = train_test_split(anomalies, random_state=22, test_size=0.9)\n",
    "y_test = pd.Series([0]*X_test.shape[0])\n",
    "X_train = X_train.append(train_anomalies)\n",
    "X_test = X_test.append(test_anomalies)\n",
    "y_test = y_test.append(pd.Series([1]*test_anomalies.shape[0]))\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "print(f'Tamaño de datos de entrenamiento: {len(X_train)} ({len(train_anomalies)} anomalías)')\n",
    "print(f'Tamaño de datos de evaluación: {len(X_test)} ({len(test_anomalies)} anomalías)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_pctg = len(train_anomalies) / len(X_train)\n",
    "anomaly_pctg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parámetros del Isolation Forest\n",
    "\n",
    "- n_estimators: Número de árboles.\n",
    "- max_samples: Número de muestras con las que se entrenará cada árbol.\n",
    "- contamination: Proporcion esperada de anomalías en el conjunto de datos. Sirve para determinar el umbral de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 50, #200, 250...\n",
    "    'max_samples': 50, #200, 250...\n",
    "    'contamination': 'auto', # 'auto', 0.1, 0.001 ...\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [   \n",
    "        ('robust_scaler', RobustScaler()),\n",
    "        ('norm', MinMaxScaler(feature_range=(0, 1))),\n",
    "    ]\n",
    ")\n",
    "\n",
    "isolation_model = IsolationForest(\n",
    "    **params,\n",
    "    warm_start=True,\n",
    "    verbose=1,\n",
    "    n_jobs=4, #para lanzar hilos concurrentes\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "isolation_model.fit(pipeline.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_thresh = - isolation_model.offset_\n",
    "print(f'Umbral de decisión: {model_thresh}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos dos funciones importantes:\n",
    "\n",
    "- score_samples: la función que nos dice la puntuación de anomalía de cada dato\n",
    "\n",
    "- predict: la función que decide si un dato es anómalo o no. Nuestra implementación del isolation forest devuelve 1 para datos normales y -1 para datos anómalos, así que los cambiamos por 0 y 1 respectivamente para que se adapte a nuestras etiquetas en la columna 'Class'.\n",
    "\n",
    "Tanto las predicciones (pred) como las puntuaciones (scores) son muy importantes para evaluar correctamente el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = isolation_model.score_samples(pipeline.transform(X_test))\n",
    "y_pred = X_test.copy()\n",
    "y_pred['pred'] = isolation_model.predict(pipeline.transform(X_test))\n",
    "y_pred['pred'].replace({1: 0, -1: 1}, inplace=True) \n",
    "y_pred['scores'] = [-score for score in scores]\n",
    "y_pred['Class'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[['Class','pred','scores']].sample(200).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "import plotly.express as px\n",
    "from plotly import graph_objects as go\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusión & métricas derivadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de confusión nos permite ver el desempeño de nuestro modelo, representando las instancias en las que acertó y falló para cada clase.\n",
    "\n",
    "Las filas de la matriz representan la clase real, y las columnas son la predicción de nuestro modelo. Como tenemos sólo dos clases, nuestra matriz tendrá una dimensión de 2x2, de la siguiente forma: \n",
    "\n",
    "<img src=\"images/cm.png\" width=\"600\"/>\n",
    "\n",
    "En la diagonal principal están todas las veces que el modelo acertó en sus predicciones para cada clase (Verdaderos Negativos - TN, y Verdaderos Positivos - VN), aquí es donde queremos que esté la mayor cantidad de instancias posibles.\n",
    "En la esquina superior derecha tenemos los Falsos Positivos (FP), es decir, las veces que el modelo dijo que algo era una anomalía cuando realmente era un dato normal.\n",
    "En la esquina inferior izquierda tenemos lo opuesto, las veces que el algoritmo predijo que una transacción era normal, cuando realmente era fraude. Estos son los Falsos Negativos (FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=y_pred['Class'], y_pred=y_pred['pred'])\n",
    "fig, ax = plt.subplots(figsize=[4, 4])\n",
    "sns.heatmap(cm, ax=ax, annot=True, fmt='g', cmap='Purples')\n",
    "ax.set_ylabel('Clase real')\n",
    "ax.set_xlabel('Predicción')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la matriz de confusión podemos sacar varias métricas importantes.\n",
    "\n",
    "- Accuracy: El porcentaje total de elementos que clasificamos correctamente. Fórmula: (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "- Precision: Para la clase positiva, permite medir la calidad de las clasificaciones del modelo. Responde a: ¿entre todos los datos que el modelo dice que son anómalos, cuales realmente lo son? Fórmula = TP / (TP + FP)\n",
    "\n",
    "- Recall: Para la clase positiva, permite medir la cantidad de veces que el modelo acierta. Responde a: ¿entre todos los datos anómalos, cuántos somos capaces de detectar? Fórmula = TP / (TP + FN)\n",
    "\n",
    "- F1: Media armónica entre precision y recall. Fórmula = (2 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "metrics = {}\n",
    "metrics['recall'] = tp / (tp + fn)  \n",
    "metrics['accuracy'] = (tp + tn) / (tp + tn + fp + fn)\n",
    "metrics['precision'] = tp / (tp + fp)\n",
    "metrics['f1'] = 2 * (\n",
    "    (metrics['precision'] * metrics['recall'])\n",
    "    / (metrics['precision'] + metrics['recall'])\n",
    ")\n",
    "pd.DataFrame.from_records(metrics, index=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la accuracy nos da, por lo general, valores superiores al resto de métricas. Esto se debe a que tenemos un dataset no balanceado, donde la clase normal tiene muchos más puntos que la anómala. La accuracy no distingue entre clases, sólo si el modelo acierta o no, por lo que se ve muy fuertemente influenciada por la clase de mayor tamaño. Por lo general, preferimos usar el precision y recall como métricas cuando estamos trabajando con datasets no balanceados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score Distribution\n",
    "\n",
    "Las métricas que hemos visto hasta ahora evalúan el modelo según sus aciertos, porque nos basamos en las etiquetas de clasificación finales: 0 o 1.\n",
    "\n",
    "Sin embargo, también podemos querer visualizar las puntuaciones de anomalía que obtienen nuestros datos, para ver cómo de bien distingue el modelo entre las dos clases, y si es capaz de separarlas.\n",
    "\n",
    "Si representamos un histograma de las puntaciones que obtiene cada clase comparadas con el umbral de clasificación de nuestro modelo (recordamos que por defecto es 0.5), obtenemos el siguiente gráfico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = -isolation_model.offset_\n",
    "hist_data = [\n",
    "    y_pred['scores'][y_pred['Class'] == 0],\n",
    "    y_pred['scores'][y_pred['Class'] == 1],\n",
    "]\n",
    "colors = ['#835AF1', '#7FA6EE']\n",
    "\n",
    "fig = ff.create_distplot(\n",
    "    hist_data,\n",
    "    ['Normal', 'Anomalía'], \n",
    "    bin_size=.015, # cambiar valor para ver con más o menos detalle la distribución de los datos\n",
    "    histnorm='probability density',\n",
    "    colors=colors\n",
    ")\n",
    "fig.add_vline(\n",
    "    x=thresh, \n",
    "    line=dict(dash='dot'),\n",
    "    annotation_text=f'  umbral de decisión={thresh:.4f}',\n",
    "    annotation=dict(font_size=9),\n",
    "    fillcolor='green'\n",
    ")\n",
    "\n",
    "fig.update_layout(height=500, width=800, title_text='Distribución de scores')\n",
    "fig.update_xaxes(title_text='Score')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si comparamos este histograma con los que habíamos obtenido al principio para las variables originales del dataset, podemos ver claras diferencias. Como mínimo, ahora no hay solape entre las partes más densas de cada curva. Podemos trazar una linea que divide ambas clases.\n",
    "\n",
    "Esencialmente, lo que consigue nuestro modelo de detección de anomalías es elaborar una función para definir una variable nueva a partir de nuestro conjunto de datos, la score, que separa lo mejor posible los valores normales de las anomalías.\n",
    "Si nuestro modelo fuese perfecto, las curvas estarían perfectamente divididas por el umbral de decisión. \n",
    "\n",
    "Podemos ver que si desplazamos nuestro umbral la matriz de confusión y las métricas obtenidas cambiarían. Para ver estos cambios en los gráficos, podemos volver a entrenar el algoritmo cambiando el parámetro 'contamination' a valores no automáticos, haciendo que cambie su threshold en función de las puntuaciones de los datos de entrenamiento.\n",
    "\n",
    "**Nota: este tipo de gráfico normaliza las curvas para que tengan dimensiones similares y se visualicen mejor sus tendencias. En realidad, tenemos muchas menos anomalías que datos normales, lo cual resultaría en un histograma con valores más pequeños."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC ROC\n",
    "\n",
    "La AUC ROC es la forma típica de cuantificar esta capacidad del modelo de separar correctamente las dos clases. El AUC es el valor del área bajo la curva ROC, que es una curva de **probabilidad**. \n",
    "Para obtenerla, vamos desplazando nuestro umbral de decisión (threshold), y observamos cómo evolucionan nuestra TPR (Tasa de Verdaderos Positivos) y FPR (Tasa de Falsos Positivos). Para obtener una AUC ideal, nuestra curva ROC pasaría por el punto [0,1], lo cual querría decir que existe un umbral de decisión que nos permite tener un 0% de falsos positivos a la vez que un 100% de verdaderos positivos.\n",
    "\n",
    "El mejor valor posible de AUC es 1, pero el peor valor no es 0 sino 0.5, porque quiere decir que el modelo tiene una probabilidad de 0.5 de acertar, la misma que si escojemos las clases al azar. Si la AUC da 0 es que el modelo está etiquetando las clases al revés (¡pero esto sigue queriendo decir que puede distinguir entre ellas!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=y_test\n",
    "scores=y_pred['scores']\n",
    "\n",
    "fpr, tpr, _ = roc_curve(labels, scores)\n",
    "thr = sum(labels) / len(labels)\n",
    "\n",
    "fig = px.area(\n",
    "    x=fpr, \n",
    "    y=tpr,\n",
    "    labels=dict(x='FPR',y='TPR'),\n",
    "    title=f'Curva ROC (AUC={auc(fpr, tpr):.4f})',\n",
    "    color_discrete_sequence=px.colors.qualitative.Pastel\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "    x=[0,1], y=[0,1], \n",
    "    line=dict(dash='dot', color='purple'), \n",
    "    name='AUC-ROC base = 0.5')\n",
    ")\n",
    "\n",
    "fig.update_layout(height=400, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra AUC nos indica que el modelo tiene una capacidad muy alta para separar entre ambas clases, independientemente del umbral de decisión que seleccionemos.\n",
    "\n",
    "En la mayor parte de modelos de ML no obtendremos una AUC perfecta, por lo que buscaremos encontrar un compromiso entre los verdaderos y falsos positivos. Depende de nosotros lo que queramos priorizar a la hora de definir las etiquetas: detectar todos los fraudes posibles a cambio de tener falsos positivos, lo cual podría tener consecuencias negativas para los usuarios legítimos de nuestras aplicaciones, o intentar reducir los falsos positivos detectando menos anomalías.\n",
    "\n",
    "En este caso, el impacto de no detectar un fraude en una transacción bancaria es mucho más alto que el de actuar sobre un falso positivo. Generalmente preferimos que el banco bloquee la transacción, nos envíe un SMS de alerta o nos llame, incluso si era una actividad válida, antes que dejar que alguien use nuestra tarjeta de crédito de forma fraudulenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
